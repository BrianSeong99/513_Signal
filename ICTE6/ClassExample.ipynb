{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Qb3XH7xpVtW"
      },
      "outputs": [],
      "source": [
        "# Import the required libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu7wZFJQpax9"
      },
      "source": [
        "*   import numpy as np: This line imports the numpy library and gives it the alias np for convenience. numpy is used for numerical computations in the code.\n",
        "\n",
        "*   import tensorflow as tf: This line imports the tensorflow library and gives it the alias tf for convenience. tensorflow is used for building and training the neural network.\n",
        "\n",
        "*   from sklearn.datasets import load_iris: This line imports the load_iris function from scikit-learn's datasets module. The load_iris function is used to load the Iris dataset.\n",
        "\n",
        "*   from sklearn.model_selection import train_test_split: This line imports the train_test_split function from scikit-learn's model_selection module. The train_test_split function is used to split the data into training and testing sets.\n",
        "\n",
        "*   from sklearn.ensemble import RandomForestClassifier: This line imports the RandomForestClassifier class from scikit-learn's ensemble module. The RandomForestClassifier class is used to train the Random Forest Classifier.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KanM3PZ-pxyb"
      },
      "outputs": [],
      "source": [
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris[\"data\"]\n",
        "y = iris[\"target\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugIduiSwp1k_"
      },
      "source": [
        "iris = load_iris(): This line loads the Iris dataset using the load_iris function. The load_iris function returns the dataset as a dictionary-like object with the keys data, target, target_names, and DESCR.\n",
        "\n",
        "X = iris[\"data\"]: This line extracts the input features from the Iris dataset and assigns it to the variable X.\n",
        "\n",
        "y = iris[\"target\"]: This line extracts the target values from the Iris dataset and assigns it to the variable y."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odfMhleJp5N3"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN-LLMajp-4f"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0): This line splits the input features and target values into training and testing sets using the train_test_split function. The test_size parameter is set to 0.2, which means 20% of the data will be used for testing and 80% of the data will be used for training. The random_state parameter is set to 0 to ensure reproducibility. The training data is assigned to X_train and y_train, and the testing data is assigned to X_test and y_test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3_TOUAhp_uS"
      },
      "outputs": [],
      "source": [
        "clf = RandomForestClassifier(random_state=0)\n",
        "clf.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAZjL2CbqFdG"
      },
      "source": [
        "clf = RandomForestClassifier(random_state=0): This line creates an instance of the RandomForestClassifier class with the random_state parameter set to 0 to ensure reproducibility.\n",
        "\n",
        "clf.fit(X_train, y_train): This line trains the Random Forest Classifier on the training data using the fit method. The fit method takes the training input features X_train and the target values y_train as input and trains the classifier on this data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQt8mXbuqJGV"
      },
      "outputs": [],
      "source": [
        "# Predict the target for the testing data using the trained Random Forest Classifier\n",
        "y_pred = clf.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNcljb4bqLv-"
      },
      "source": [
        "y_pred = clf.predict(X_test): This line predicts the target values for the testing data using the trained Random Forest Classifier and the predict method. The predict method takes the testing input features X_test as input and returns the predicted target values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olOC6aJGqPjN"
      },
      "outputs": [],
      "source": [
        "# Evaluate the Random Forest Classifier using accuracy score\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print(\"Accuracy of Random Forest Classifier:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wd_CGQShqScX"
      },
      "source": [
        "accuracy = clf.score(X_test, y_test): This line evaluates the accuracy of the Random Forest Classifier on the testing data using the score method. The score method takes the testing input features X_test and target values y_test as input and returns the accuracy score.\n",
        "\n",
        "print(\"Accuracy of Random Forest Classifier:\", accuracy): This line prints the accuracy of the Random Forest Classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZCUwU3wqRr6"
      },
      "outputs": [],
      "source": [
        "# Convert the target values to one-hot encoded tensors\n",
        "y_train = tf.keras.utils.to_categorical(y_train)\n",
        "y_test = tf.keras.utils.to_categorical(y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLUFXMRvqXFo"
      },
      "source": [
        "y_train = tf.keras.utils.to_categorical(y_train): This line converts the target values for the training set to one-hot encoded tensors using TensorFlow's to_categorical function.\n",
        "\n",
        "y_test = tf.keras.utils.to_categorical(y_test): This line converts the target values for the testing set to one-hot encoded tensors using TensorFlow's to_categorical function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQH3MNI6qZTZ"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(64, activation='relu', input_shape=(4,)))\n",
        "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(3, activation='softmax'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qi1hBBYCqbDp"
      },
      "source": [
        "model = tf.keras.Sequential(): This line creates an instance of TensorFlow's Sequential class, which is used to define a sequential neural network model.\n",
        "\n",
        "model.add(tf.keras.layers.Dense(64, activation='relu', input_shape=(4,))): This line adds a dense layer to the model using TensorFlow's Dense layer. The dense layer has 64 units, a relu activation function, and the input shape is set to (4,) as the input features have 4 dimensions.\n",
        "\n",
        "model.add(tf.keras.layers.Dense(64, activation='relu')): This line adds another dense layer to the model with 64 units and a relu activation function.\n",
        "\n",
        "model.add(tf.keras.layers.Dense(3, activation='softmax')): This line adds the final dense layer to the model with 3 units and a softmax activation function. The softmax activation function is used as the target values are one-hot encoded and represent a multi-class classification problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDAdGz1oqdmk"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maNXFLuzqfii"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']): This line compiles the model using the compile method. The loss parameter is set to 'categorical_crossentropy' as the target values are one-hot encoded and represent a multi-class classification problem. The optimizer parameter is set to 'adam' to use the Adam optimizer. The metrics parameter is set to ['accuracy'] to track the accuracy during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Sq5TACiqibv"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OylSLkaqk1v"
      },
      "source": [
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test)): This line trains the model using the fit method. The fit method takes the training input features X_train and target values y_train as input and trains the model for 100 epochs with a batch size of 32. The validation_data parameter is set to (X_test, y_test) to use the testing data for validation during training. The training history is stored in the history variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkI2_tvBqoEq"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cSNaHsKquat"
      },
      "source": [
        "loss, accuracy = model.evaluate(X_test, y_test): This line evaluates the model on the testing data using the evaluate method. The evaluate method takes the testing input features X_test and target values y_test as input and returns the loss and accuracy.\n",
        "\n",
        "print(\"Test Loss:\", loss): This line prints the test loss.\n",
        "\n",
        "print(\"Test Accuracy:\", accuracy): This line prints the test accuracy."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
